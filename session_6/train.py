# -*- coding: utf-8 -*-
"""train

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lsm4l5NGBjIND7GdAChlBFGyw4KBtFpX
"""

import torch.nn.Functional as F
class Train:
  def __init__(self,model,dataloader,optimizer,device):

    self.model=model
    self.device=device
    self.dataloader=dataloader
    self.optimizer=optimizer
    self.train_epoch_loss=[]
    self.train_epoch_accuracy=[]
    
  def train(data_loader,model,optimizer,device):
    self.model.train()
    progressbar= tqdm(self.data_loader)

    running_loss=0.0
    running_correct=0

    for batchidx,(images,targets) in enumerate(progressbar):
      images,targets = images.to(device),targets.to(device)

      # set gradients as 0
      optimizer.zero_grad()

      # forward pass/prediction
      outputs= self.model(images)

      # Calculate loss
      loss= F.nll_loss(outputs,targets)

      # accumulate loss of every batch
      running_loss+=loss.item() 

      # Backpropogation
      loss.backward()
      optimizer.step()

      predicted_labels= outputs.argmax(dim=1,keepdims=True)
      running_correct+=predicted_labels.eq(targets.view_as(predicted_labels)).sum().item() 
      

    loss= running_loss/len(data_loader.dataset)    
    accuracy=100*running_correct/len(data_loader.dataset)

    self.train_epoch_loss.append(loss)
    self.train_epoch_accuracy.append(accuracy)

    print(f'LR={optimizer.param_groups[0]["lr"]}  | Training_loss:{loss}  | Training Accuracy {accuracy:.2f}')
    return loss, accuracy