# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RAW7o8aCrCdTU1MBBytSod6jwRS1BR_0
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""
Training a Classifier
=====================

This is it. You have seen how to define neural networks, compute loss and make
updates to the weights of the network.

Now you might be thinking,

What about data?
----------------

Generally, when you have to deal with image, text, audio or video data,
you can use standard python packages that load data into a numpy array.
Then you can convert this array into a ``torch.*Tensor``.

-  For images, packages such as Pillow, OpenCV are useful
-  For audio, packages such as scipy and librosa
-  For text, either raw Python or Cython based loading, or NLTK and
   SpaCy are useful

Specifically for vision, we have created a package called
``torchvision``, that has data loaders for common datasets such as
Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,
``torchvision.datasets`` and ``torch.utils.data.DataLoader``.

This provides a huge convenience and avoids writing boilerplate code.

For this tutorial, we will use the CIFAR10 dataset.
It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,
‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of
size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.

.. figure:: /_static/img/cifar10.png
   :alt: cifar10

   cifar10


Training an image classifier
----------------------------

We will do the following steps in order:

1. Load and normalizing the CIFAR10 training and test datasets using
   ``torchvision``
2. Define a Convolution Neural Network
3. Define a loss function
4. Train the network on the training data
5. Test the network on the test data

1. Loading and normalizing CIFAR10
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Using ``torchvision``, it’s extremely easy to load CIFAR10.

"""

!rm -rf /content/EVA-Computer-Vision-Using-Pytorch

! git clone https://github.com/minakshimathpal/EVA-Computer-Vision-Using-Pytorch.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/EVA-Computer-Vision-Using-Pytorch/session_6

import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset,DataLoader
import albumentations
import albumentations.pytorch
from torchvision import datasets,transforms
import random
from PIL import Image
import numpy as np
from matplotlib import pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, SubsetRandomSampler
seed=42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
import warnings
warnings.filterwarnings("ignore")

"""The output of torchvision datasets are PILImage images of range [0, 1].
We transform them to Tensors of normalized range [-1, 1].


"""

# imports from utility modules
from utils import get_dataloader_cifar10
from train import Train
from val import Validate

# class Cifar10Dataset(torchvision.datasets.CIFAR10):
#   def __init__(self,root="./data",train=True,download=True,transform=None):
#     super().__init__(root=root,train=train,download=download,transform=transform)

#   def __getitem__(self,index):
#     image, label= self.data[index],self.targets[index]
    
#     # apply the transformations
#     if self.transform is not None :
#       transformed = self.transform(image=image) 
#       image=transformed["image"]

#       return (image, label)

# def get_dataloader_cifar10(dataloader_args,
#                            validation_fraction=None,
#                            train_transform=None,
#                            test_transform=None,
#                            ):
  
#   train_set= Cifar10Dataset(root="./data",train=True,download=True,transform=train_transform)
#   valid_set= Cifar10Dataset(root="./data",train=True,download=True,transform=test_transform)
#   test_set=  Cifar10Dataset(root="./data",train=False,download=True,transform=test_transform)
#   print(len(test_set))
#   if validation_fraction is not None:
#       num= int(validation_fraction * 50000)
#       train_indices= torch.arange(0,50000 - num)
#       valid_indices= torch.arange(50000 - num,50000)

#       train_sampler = SubsetRandomSampler(train_indices)
#       valid_sampler = SubsetRandomSampler(valid_indices) 

#       valid_loader = torch.utils.data.DataLoader(dataset=valid_set,
#                                                 **dataloader_args,
#                                                 sampler=valid_sampler,
                                               
#                                                 )
      
#       train_loader = torch.utils.data.DataLoader(dataset= train_set,
#                                                 **dataloader_args,                                                
#                                                 sampler=train_sampler,
                                              
#                                                 )
                                                
#   else:
#       print("inside else ")
#       train_loader = torch.utils.data.DataLoader(dataset=train_set,
#                                                **dataloader_args,                                                
                                                
#                                                 )                                              

  
#   test_loader = DataLoader(dataset=test_set,
#                            **dataloader_args,
#                            )
#   for i, (features, targets) in enumerate(test_loader):
#     print(targets.size())
#     break
  

#   if validation_fraction is None:
#         print("Returning train and test loader")
#         return train_loader, test_loader
#   else:
#         return train_loader, valid_loader, test_loader

# CUDA ?
is_cuda = torch.cuda.is_available()
print("CUDA available ",is_cuda)

seed=42
if is_cuda:
  torch.cuda.manual_seed(seed)
np.random
device= torch.device("cuda" if is_cuda  else "cpu")

# Data Loader arguments
dataloader_args= dict(shuffle=True,batch_size=128,num_workers=4,pin_memory=4) if is_cuda else dict(shuffle=True,batch_size=64)

# Define dataset class(specifically needed for Albumnentations)
# Data
print('==> Preparing data..')


train_transforms=albumentations.Compose([albumentations.HorizontalFlip(p=0.5),
                                          albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1,
                                                                          
                                                                          rotate_limit=45, p=0.5),
                                          albumentations.CoarseDropout(max_holes = 1,
                                                                       max_height=12,
                                                                       max_width=12,
                                                                       min_holes = 1,
                                                                       min_height=12, 
                                                                       min_width=12,
                                                                       fill_value=127.5),
                                         
                                          albumentations.Normalize(mean=(0.5,0.5,0.5),std=(0.2470, 0.2435, 0.2616),p =1.0),
                                         
                                          albumentations.pytorch.ToTensorV2()])

test_transforms= albumentations.Compose([albumentations.Normalize(mean=(0.5,0.5,0.5),std=(0.2470, 0.2435, 0.2616),p =1.0),
                                         albumentations.pytorch.ToTensorV2()])

# Get the dataset
trainloader,testloader = get_dataloader_cifar10(dataloader_args,
                                  validation_fraction=None,
                                  train_transform=train_transforms,
                                  test_transform=test_transforms)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

print("Total Training Samples:",len(trainloader.dataset))
print("Total Test Samples    :",len(testloader.dataset))

images,labels= next(iter(trainloader))
print("Batch Size            :",images.size(0))
print("Images Size           :",images[0].size())

"""Let us show some of the training images, for fun.


"""

# lets plot the some images
import matplotlib.pyplot as plt
import numpy as np
plt.figure(figsize=(10,10))
# functions to show an image


def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    print(npimg.shape)
    plt.imshow(np.transpose(npimg, (1, 2, 0)))


# get some random training images
dataiter = iter(trainloader)
images, labels = next(dataiter)

# show images
imshow(torchvision.utils.make_grid(images[:10],nrow=10))
# print labels
print(' '.join('%5s' % classes[labels[j]] for j in range(10)))

dropout_value=0.1
class Net(nn.Module):
  def __init__(self):
    super(Net,self).__init__()   

    # Input Block
    self.convblock_0 = nn.Sequential(
                       nn.Conv2d(in_channels=3,out_channels=32,kernel_size=(3,3),dilation=2,stride=1,padding=2,bias=False,),
                       nn.ReLU(),
                       nn.BatchNorm2d(32),
                       nn.Dropout(dropout_value), # 32X32X32 | RF=5

                       nn.Conv2d(in_channels=32,out_channels=32,kernel_size=(3,3),dilation=2,stride=1,padding=2,bias=False,),
                       nn.ReLU(),
                       nn.BatchNorm2d(32),
                       nn.Dropout(dropout_value), # 32X32x#2 |RF=9

                       nn.Conv2d(in_channels=32,out_channels=64,kernel_size=(3,3),dilation=2,stride=1,padding=2,bias=False,),
                       nn.ReLU(),
                       nn.BatchNorm2d(64),
                       nn.Dropout(dropout_value), # 32X32X64 |RF= 13
                      )  
    
    # depthwise seperable Convolution 1
    self.convblock_1 = nn.Sequential(
        
                       nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(3,3),stride=(2,2),dilation=2,padding=1,bias=False,),# maxpool added after RF >11
                       nn.ReLU(),
                       nn.BatchNorm2d(64),
                       nn.Dropout(dropout_value), # 15X15X64 |RF=17

                       nn.Conv2d(in_channels=64,out_channels=64,groups=64,kernel_size=(3,3),stride=(1,1),dilation=2,padding=2,bias=False,),
                       nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),
                       nn.ReLU(),
                       nn.BatchNorm2d(64), 
                       nn.Dropout(dropout_value), # 15X15X64 | RF=25                                       
                       # pointwise   

                       nn.Conv2d(in_channels=64,out_channels=64,groups=64,kernel_size=(3,3),dilation=2,stride=(1,1),padding=2,bias=False,),
                       nn.Conv2d(in_channels=64,out_channels=32,kernel_size=(1,1),padding=0,bias=False,),
                       nn.ReLU(),
                       nn.BatchNorm2d(32),   
                       nn.Dropout(dropout_value), # 15X15X64 | RF=33
                       
                      #nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(3,3),stride=(1,1),dilation=2,padding=1,bias=False,),
                      # #  nn.Conv2d(in_channels=64,out_channels=64,groups=64,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),
                      # #  nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(1,1),padding=0,bias=False,),
                      #  nn.ReLU(),
                      #  nn.BatchNorm2d(64),   
                      #  nn.Dropout(dropout_value) , # 15X15X64 | RF=29                                                         
                       )        

    # depthwise seperable Convolution 2
    self.convblock_2 = nn.Sequential(
        
                       nn.Conv2d(in_channels=32,out_channels=32,kernel_size=(3,3),stride=(2,2),dilation=2,padding=1,bias=False,),
                       nn.ReLU(),
                       nn.BatchNorm2d(32),   
                       nn.Dropout(dropout_value), # 7X7X32 | RF=41

                       nn.Conv2d(in_channels=32,out_channels=32,groups=32,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),
                       nn.Conv2d(in_channels=32,out_channels=64,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),
                       nn.ReLU(),
                       nn.BatchNorm2d(64),  
                       nn.Dropout(dropout_value),  # 7X7X64| RF=49                                      
                      # pointwise   

                       nn.Conv2d(in_channels=64,out_channels=64,groups=64,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),
                       nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),
                       nn.ReLU(),
                       nn.BatchNorm2d(64),  # pointwise 
                       nn.Dropout(dropout_value) # 7X7X64 | RF=57

                      ) 

    # depthwise seperable Convolution 2
    self.convblock_3 = nn.Sequential(
        
                       #Maxpooling
                       nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(3,3),dilation=2,stride=(2,2),padding=2,bias=False),
                       nn.ReLU(),
                       nn.BatchNorm2d(64),  
                       nn.Dropout(dropout_value),  # 4X4X64  | RF=73

                       nn.Conv2d(in_channels=64,out_channels=64,groups=64,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),                       
                       nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),
                       nn.ReLU(),
                       nn.BatchNorm2d(64),  
                       nn.Dropout(dropout_value), #  4X4X128 | RF=89                                  
                     
                       nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(3,3),dilation=2,stride=(1,1),padding=2,bias=False),
                       nn.ReLU(),
                       nn.BatchNorm2d(64),  
                       nn.Dropout(dropout_value),  #  4X4X64  | RF=105

                      #  nn.Conv2d(in_channels=128,out_channels=128,groups=128,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),
                      #  nn.Conv2d(in_channels=128,out_channels=128,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),
                      #  nn.ReLU(),
                      #  nn.BatchNorm2d(128),  # pointwise 
                      #  nn.Dropout(dropout_value), # 3x3x128 | RF= 101

                      #  nn.Conv2d(in_channels=128,out_channels=128,groups=128,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),
                      #  nn.Conv2d(in_channels=128,out_channels=128,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),
                      #  nn.ReLU(),
                      #  nn.BatchNorm2d(128),  # pointwise 
                      #  nn.Dropout(dropout_value),

                       #Maxpooling
                       nn.Conv2d(in_channels=64,out_channels=10,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,)
                       ) # 4X4X10 | RF=121

    # self.convblock_4 = nn.Sequential(
        
                       
    #                    nn.Conv2d(in_channels=64,out_channels=64,groups=64,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),
    #                    nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),
    #                    nn.ReLU(),
    #                    nn.BatchNorm2d(128),  
    #                    nn.Dropout(dropout_value),                                        
    #                    # pointwise                        

    #                    nn.Conv2d(in_channels=128,out_channels=128,groups=128,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),
    #                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),
    #                    nn.ReLU(),
    #                    nn.BatchNorm2d(128),  # pointwise 
    #                    nn.Dropout(dropout_value),

    #                    #Maxpooling
    #                    nn.Conv2d(in_channels=128,out_channels=10,kernel_size=(3,3),stride=(2,2),dilation=2,padding=1,bias=False,)
    #                    ) # 11X11X64 | RF=13 

    self.gap = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
        )
    self.convblock_4 = nn.Sequential( 
                         nn.Conv2d(in_channels=10,out_channels=10,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,) 
    )
       

  def forward(self, x):
        x = self.convblock_0(x)
        x = self.convblock_1(x)
        x = self.convblock_2(x)
        x = self.convblock_3(x)
        x = self.gap(x) 
        x = self.convblock_4(x)     
        x = x.view(-1, 10)
        return F.log_softmax(x, dim=1)
                 
                            
                                                               
                                  

                               
                                
                                
                                
                                   
                               
                               
    # # Convolution 2
    # self.convblock_2 = nn.Sequential(
    #                            nn.Conv2d(in_channels=32,
    #                                      out_channels=64,
    #                                      kernel_size=(3,3),
    #                                      padding=1,
    #                                      dilation=2,
    #                                      bias=False,),
    #                            nn.ReLU(),
    #                            nn.BatchNorm2d(64),
    #                            nn.Dropout(0.05),      
                                     
    #                            nn.Conv2d(in_channels=64,
    #                                      out_channels=64,
    #                                      kernel_size=(3,3),
    #                                      padding=1,
    #                                      dilation=2,
    #                                      bias=False,),      
                               
    #                            nn.ReLU(),
    #                            nn.BatchNorm2d(64),
    #                            nn.Dropout(0.05),           
    #                           )
    # # Transition block with Convolution with stride of 2 to replace Maxpooling
    # self.trans1  = nn.Sequential(
    #                               nn.Conv2d(in_channels=64,
    #                                      out_channels=64,
    #                                      kernel_size=(1,1),
    #                                      padding=1,
    #                                      bias=False,),
                                     
    #                               nn.Conv2d(in_channels=64,
    #                                      out_channels=64,
    #                                      kernel_size=(3,3),
    #                                      padding=1,
    #                                      stride=(2,2),
    #                                      bias=False,),                                                     
                                 
                                  
    #                               )
    # # Depthwise seperable convolution which is the combination of Depthwise and pointwise convolution
    # self.convblock_3 =  nn.Sequential(
    #                               nn.Conv2d(64, 64, kernel_size=3, padding=1, groups=64), # Depthwise convolution
    #                               nn.Conv2d(64, 10, kernel_size=1))  # Pointwise convolution
    # # GAP Layer
    # self.gap = nn.Sequential(nn.AvgPool2d(kernel_size=12))  # output_size = 1

    # self.convblock_4 = nn.Sequential(
    #             nn.Conv2d(
    #             in_channels=10,
    #             out_channels=10,
    #             kernel_size=(1, 1),
    #             padding=0,
    #             bias=False,
    #         ),
    #     )
    
  # def forward(self,x):
        
  #       x = self.convblock_0(x)
  #       x = self.convblock_1(x)
  #       x = self.convblock_2(x)
  #       x = self.trans1(x)
  #       x = self.convblock_3(x)
  #       x = self.gap(x)
  #       x = self.convblock_4(x)  
        
  #       x = x.view(-1, 10)

  #       return F.log_softmax(x, dim=-1)

from torchsummary import summary
model=Net().to(device)
summary(model,input_size=(3,32,32))

def train(data_loader,model,optimizer,device):
    model.train()
    progressbar= tqdm(data_loader)

    running_loss=0.0
    running_correct=0

    for batchidx,(images,targets) in enumerate(progressbar):
      images,targets = images.to(device),targets.to(device)

      # set gradients as 0
      optimizer.zero_grad()

      # forward pass/prediction
      outputs= model(images)

      # Calculate loss
      loss= F.nll_loss(outputs,targets)

      # accumulate loss of every batch
      running_loss+=loss.item() # self.train_losses.append(loss.item()) I am doinf it at epochs

      # Backpropogation
      loss.backward()
      optimizer.step()

      predicted_labels= outputs.argmax(dim=1,keepdims=True)
      running_correct+=predicted_labels.eq(targets.view_as(predicted_labels)).sum().item() 
      

    loss= running_loss/len(data_loader.dataset)
    accuracy=100*running_correct/len(data_loader.dataset)

    print(f'LR={optimizer.param_groups[0]["lr"]}  Training_loss:{loss}  Training Accuracy {accuracy:.2f}')
    return loss, accuracy


from tqdm import tqdm

def test(data_loader,model,optimizer,device):
    model.eval()
    progress_bar= tqdm(data_loader)
    running_loss=0.0
    running_correct= 0

    with torch.no_grad():
      for batch_idx,(images,targets) in enumerate(progress_bar):
      # get samples
        images,targets = images.to(device),targets.to(device)

        # forward pass/prediction
        outputs= model(images)
#
        # Calculate loss
        loss= F.nll_loss(outputs,targets)

      # accumulate loss of every batch
        running_loss+=loss.item() # self.train_losses.append(loss.item()) I am doinf it at epochs

        predicted_labels= outputs.argmax(dim=1,keepdims=True)
        running_correct+=predicted_labels.eq(targets.view_as(predicted_labels)).sum().item() 

      loss= running_loss/len(data_loader.dataset)
      accuracy=100*running_correct/len(data_loader.dataset)
      print(f"Validation Loss {loss} Validation Accuracy {accuracy:.2f}")
      return loss,accuracy

import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau

Epochs=200
optimizer=optim.SGD(model.parameters(),lr=.01,momentum=0.9)
criterion=nn.NLLLoss()
# scheduler= ReduceLROnPlateau(optimizer,threshold=0.0001,patience=1,factor=.215,mode='max')

train_losses = []
train_accuracies = []
epoch_train_accuracies = [] 
train_loss,train_accuracy,test_loss,test_accuracy=[],[],[],[]

for epoch in range(Epochs):
  print(f"EPOCH {epoch}")
  train_epoch_loss,train_epoch_accuracy = train(trainloader,model,optimizer,device)
  val_epoch_loss,val_epoch_accuracy = test(testloader,model,optimizer,device)  
  
  train_loss.append(train_epoch_loss)
  train_accuracy.append(train_epoch_accuracy)

  test_loss.append(val_epoch_loss)
  test_accuracy.append(val_epoch_accuracy)

# test_losses,test_accuracies=[],[]

# for epoch in range(Epochs):
#   print(f"EPOCH {epoch}")
#   train_loss= train(model, trainloader, optimizer, criterion, device, epoch)
#   test_loss= test(model, testloader, criterion,device)

print("Max Traing Accuracy Reached:",max(train_accuracy))
print("Maximum Test Accurcay Reached:",max(test_accuracy))

incorrect_samples = []
predicted_class=[]
correct_class=[]

with torch.no_grad():
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        output = model(images)
        _, pred = torch.max(output, 1)
        for i in range(len(pred)):
            if pred[i] != labels[i]:
                incorrect_samples.append(images[i])
                predicted_class.append(classes[pred[i]])
                correct_class.append(classes[labels[i]])

# Plot the misclassified images
means = [0.4914, 0.4822, 0.4465]
stds = [0.2470, 0.2435, 0.2616]

fig = plt.figure(figsize=(20, 10))
fig,ax = plt.subplots(4,5,figsize=(8,8))
for i,axes in enumerate(ax.ravel()):
    misclassified_sample = incorrect_samples[i]
    # denormalize the image
    for channel in range(misclassified_sample.shape[0]):
      misclassified_sample[channel]=misclassified_sample[channel]*stds[channel]+means[channel]   
    npimg = misclassified_sample.cpu().numpy()
    axes.imshow(np.transpose(npimg, (1, 2, 0)), cmap="gray")
    axes.set_title("Correct class: {}\nPredicted class: {}".format(correct_class[i], predicted_class[i]))
    print(i)
    plt.tight_layout()
    plt.show()

len(incorrect_samples)



import seaborn as sns
plt.figure(figsize=(6,6))
sns.lineplot(y=train_accuracy,x=range(len(train_accuracy)),label="Training Accuracy")
sns.lineplot(y=test_accuracy,x=range(len(test_accuracy)),label="Test Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.show()

plt.figure(figsize=(6,6))
sns.lineplot(y=train_loss,x=range(len(train_loss)),label="Training Loss")
sns.lineplot(y=test_loss,x=range(len(test_loss)),label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()