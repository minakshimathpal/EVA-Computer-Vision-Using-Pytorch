# -*- coding: utf-8 -*-
"""train.py.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18fYSq4xoIiNSR0vQLrYDiS5fEtNMnsA7
"""

from tqdm import tqdm

# trainimng block

def train(data_loader,model,optimizer):
    model.train()
    progressbar= tqdm(data_loader)

    running_loss=0.0
    running_correct=0

    for batchidx,(images,target) in enumerate(progressbar):
      images,targets = images.to(device),targets.to(device)

      # forward pass
      logits= model(images)

      loss= F.nll_loss(logits,targets)
      # accumulate loss of every batch
      running_loss+=loss.item()

      predicted_labels= logits.argmax(dim=1,keepdims=True)
      running_correct+=(predicted_labels == targets).cpu().sum().item()

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

    loss= running_loss/len(data_loader.dataset)
    accuracy=100*(running_correct/len(data_loader.dataset)) 

    print(f'LR={optimizer.param_groups[0]["lr"]}  Training_loss:{loss}  Training Accuracy {accuracy:.2f}')
    return loss, accuracy



def test(data_loader,model,optimizer):
    model.eval()
    progress_bar= tqdm(data_loader)
    running_loss=0.0
    running_correct= 0

    with torch.no_grad():
      for batch_idx,(images,target) in enumerate(progress_bar):
      # get samples
        images,target= images.to(device),target.to(device)

        logits= model(images)
        loss=F.nll_loss(logits,target)
        running_loss+=loss.item()
        predicted_labels= logits.argmax(dim=1,keepdims=True) # get the index of the max log-probability
        running_correct+=(predicted_labels== targets).cpu().sum().item()

      loss= running_loss/len(data_loader.dataset)
      accuracy=100*(running_correct/len(data_loader.dataset)) 
      print(f"Validation Loss {loss} Validation Accuracy {accuracy:.2f}")
      return loss,accuracy