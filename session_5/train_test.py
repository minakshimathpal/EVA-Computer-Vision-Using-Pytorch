# -*- coding: utf-8 -*-
"""train_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C1vULZyZhoRXBs0L6o3lgAVMgII4gEGZ
"""

import torch
import numpy as np
import pandas as pd
seed=42
import os
from model import Net
import seaborn as sns
torch.manual_seed(seed)
from model import Net
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchsummary import summary
from torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau
from typing import Union,List
import argparse
import numpy as np
from torchvision import datasets,transforms
import matplotlib.pyplot as plt
from torchsummary import summary
from torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau
seed=42
torch.manual_seed(seed)


"""### Target
1. Get the setup
2. Set Transforms
3. Set the Data Loader
4. Set the skeleton right
5. Set Training and testing loop
6. Results:

## Data Transformations

We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise. 

Here is the list of all the transformations which come pre-built with PyTorch

1.   Compose
2.   ToTensor
3.   ToPILImage
4. Normalize
5. Resize
6. Scale
7. CenterCrop
8. Pad
9. Lambda
10. RandomApply
11. RandomChoice
12. RandomOrder
13. RandomCrop
14. RandomHorizontalFlip
15. RandomVerticalFlip
16. RandomResizedCrop
17. RandomSizedCrop
18. FiveCrop
19. TenCrop
20. LinearTransformation
21. ColorJitter
22. RandomRotation
23. RandomAffine
24. Grayscale
25. RandomGrayscale
26. RandomPerspective
27. RandomErasing

You can read more about them [here](https://pytorch.org/docs/stable/_modules/torchvision/transforms/transforms.html)
"""

## Train Phase transformation
train_transforms= transforms.Compose([transforms.ToTensor(),
                                      transforms.Normalize((0.1307,), (0.3081,)),
                                      transforms.RandomRotation(degrees=(-7,7),fill=(1,))
                                      ])

test_transforms= transforms.Compose([transforms.ToTensor(),
                                     transforms.Normalize((0.1307,),(0.3081))])



## Get the Dataset and create train and test split

train_set= datasets.MNIST(root="./data",train=True,download=True,transform= train_transforms)

test_set= datasets.MNIST(root="./data",train=False,download=True,transform=test_transforms)

"""### Data Loader Arguments & Test and Train Arguments"""

# CUDA?
is_cuda= torch.cuda.is_available()
print("CUDA available",is_cuda)

# for reproduciability
if is_cuda:
  torch.cuda.manual_seed(seed)
device= torch.device("cuda" if is_cuda else "cpu")

## Data Loader Arguments

dataloader_args= dict(shuffle=True,batch_size=128,num_workers=4,pin_memory=4) if is_cuda else dict(shuffle=True,batch_size=64)

# train Loader

train_loader= torch.utils.data.DataLoader(train_set,**dataloader_args)
test_loader= torch.utils.data.DataLoader(test_set,**dataloader_args)

### Data Statistics
train_data=train_set.transform(train_set.data.cpu().numpy())
print("-Numpy Shape:",train_data.numpy().shape)
print("-Tensor Shape ",train_data.shape)
print(' - min:', torch.min(train_data))
print(' - max:', torch.max(train_data))
print(' - mean:', torch.mean(train_data))
print(' - std:', torch.std(train_data))
print(' - var:', torch.var(train_data))

# lets visualize some  images
images,labels= next(iter(train_loader))
num_of_images=60
for index in range(1,num_of_images+1):
  plt.subplot(6, 10, index)
  plt.axis('off')
  plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')

from torchsummary import summary
from torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau
from typing import Union,List
import argparse

parser= argparse.ArgumentParser(description="Digit Classifier")
parser.add_argument("-n","--norm_type",default="BN",type=str,help="Enter 'BN' for Batch norm 'LN' for layer norm and 'GN' for Group Norm")
parser.add_argument("-E","--EPOCHS",default=20,type=int)
parser.add_argument("-ng","--n_groups",default=0,type=int,help="no of groups you want for group normalization.Pass 1 if using layer norm")
parser.add_argument("-dv","--dropout_value",default=0.05,type=float,help="dropout value")
args= parser.parse_args()

# training block
from tqdm import tqdm

def train(data_loader,model,optimizer):
    model.train() # set the mode(train/test)
    progress_bar= tqdm(data_loader)
    running_loss=0.0
    running_correct= 0

    # iterate over data- Access batch_id and images and labels
    for batch_idx,(images,target) in enumerate(progress_bar):
      # get samples
      images,target= images.to(device),target.to(device)      
      
      # compute prediction error
      y_pred= model(images)
      loss= F.nll_loss(y_pred,target)
      running_loss+=loss.item() # accumulating loss across every batch
      preds= y_pred.argmax(dim=1,keepdims=True) # get the index of the max log-probability
      running_correct+=preds.eq(target.view_as(preds)).cpu().sum().item()

      # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. 
      # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.
      # Backpropogation  
      optimizer.zero_grad()          
      loss.backward()
      optimizer.step()     

    # calculate Loss and accuracy accross every epoch
    loss= running_loss/len(data_loader.dataset)
    accuracy=100*(running_correct/len(data_loader.dataset))  

    print(f'LR={optimizer.param_groups[0]["lr"]} Training Loss {loss} Training Accuracy {accuracy:.2f}')    
    return loss, accuracy

def test(data_loader,model,optimizer):
  model.eval()
  progress_bar= tqdm(data_loader)
  running_loss=0.0
  running_correct= 0

  with torch.no_grad():
    for batch_idx,(images,target) in enumerate(progress_bar):
      # get samples
      images,target= images.to(device),target.to(device)

      # prediction
      y_preds= model(images)
      loss=F.nll_loss(y_preds,target)
      running_loss+=loss.item()
      preds= y_preds.argmax(dim=1,keepdims=True) # get the index of the max log-probability
      running_correct+=preds.eq(target.view_as(preds)).cpu().sum().item()

    loss= running_loss/len(data_loader.dataset)
    accuracy=100*(running_correct/len(data_loader.dataset)) 
    print(f"Validation Loss {loss} Validation Accuracy {accuracy:.2f}")
    return loss,accuracy
  
x_epoch = []
fig = plt.figure()
ax0 = fig.add_subplot(121, title="Loss")
ax1 = fig.add_subplot(122, title="Accuracy")

def plot(epochs,**kargs):    
    ax0.plot(epochs, losses['train'], 'bo-', label='train loss')
    ax0.plot(epochs, losses['val'], 'ro-', label='val loss')
    ax1.plot(epochs, accuracy['train'], 'bo-', label='train accuracy')
    ax1.plot(epochs, accuracy['val'], 'ro-', label='val accuracy')
    if epochs[0] == 0:
        ax0.legend()
        ax1.legend()
    fig.savefig(os.path.join('./LossGraphs', 'train.jpg'))

if __name__=="__main__":
  
  model=Net(args.norm_type,args.n_groups,args.dropout_value).to(device)
  optim_adam=optim.Adam(model.parameters(),lr=2.15E-02)
  scheduler= ReduceLROnPlateau(optim_adam,threshold=0.0001,patience=1,factor=.215,mode='max')
  summary(model,input_size=(1,28,28))
 
  losses, accuracy={},{}
  losses["train"]=[]
  losses["val"]=[]
  accuracy["train"]=[]
  accuracy["val"]=[]
  for epoch in range(args.EPOCHS):
    print(f"EPOCH {epoch}")
    train_epoch_loss,train_epoch_accuracy = train(train_loader,model,optim_adam)
    val_epoch_loss,val_epoch_accuracy = test(test_loader,model,optim_adam)

    scheduler.step(val_epoch_accuracy)
    losses["train"].append(train_epoch_loss)
    accuracy["train"].append(train_epoch_accuracy)

    losses["val"].append(val_epoch_loss)
    accuracy["val"].append(val_epoch_accuracy)
  print(len(losses["train"]))
  plot(range(args.EPOCHS),losses=losses,accuracy=accuracy)  
  # plt.figure(figsize=(8,8))
  # sns.lineplot(y=train_accuracy,x=range(len(train_accuracy)),label="Training Accuracy")
  # sns.lineplot(y=val_accuracy,x=range(len(val_accuracy)),label="Validation Accuracy")
  # plt.xlabel("Epochs")
  # plt.ylabel("Accuracy")
  # plt.show()

  # plt.figure(figsize=(8,8))
  # sns.lineplot(y=train_losses,x=range(len(train_losses)),label="Training Loss")
  # sns.lineplot(y=val_losses,x=range(len(val_losses)),label="Validation Loss")
  # plt.xlabel("Epochs")
  # plt.ylabel("Loss")
  # plt.show()

# class Execute_model:
#   def __init__(self,norm_type:Union[Batch_Norm_Net,Group_Norm_Layer_Norm_Net],device):
#     self.norm_type=norm_type
#     self.device=device

#   def __call__(self,data_loader):